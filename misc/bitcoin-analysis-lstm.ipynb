{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nh/7t7m53hj5cd8hw1bcq95pvpm0000gn/T/ipykernel_18005/2965158937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment-bitcoin.csv')\n",
    "df = df.rename(columns = {'Unnamed: 0': 'timestamp'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple metrics study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Polarity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sensitivity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet_vol'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close_Price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting outliers / sudden spikes in our close prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(signal, treshold = 2.0):\n",
    "    detected = []\n",
    "    for i in range(len(signal)):\n",
    "        if np.abs(signal[i]) > treshold:\n",
    "            detected.append(i)\n",
    "    return detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.copy(df['Close_Price'].values)\n",
    "std_signal = (signal - np.mean(signal)) / np.std(signal)\n",
    "s = pd.Series(std_signal)\n",
    "s.describe(percentiles = [0.25, 0.5, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = detect(std_signal, 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(np.arange(len(signal)), signal)\n",
    "plt.plot(\n",
    "    np.arange(len(signal)),\n",
    "    signal,\n",
    "    'X',\n",
    "    label = 'outliers',\n",
    "    markevery = outliers,\n",
    "    c = 'r',\n",
    ")\n",
    "plt.xticks(\n",
    "    np.arange(len(signal))[::15], df['timestamp'][::15], rotation = 'vertical'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax = MinMaxScaler().fit(df[['Polarity', 'Sensitivity', 'Close_Price']])\n",
    "scaled = minmax.transform(df[['Polarity', 'Sensitivity', 'Close_Price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(np.arange(len(signal)), scaled[:, 0], label = 'Scaled polarity')\n",
    "plt.plot(np.arange(len(signal)), scaled[:, 1], label = 'Scaled sensitivity')\n",
    "plt.plot(np.arange(len(signal)), scaled[:, 2], label = 'Scaled closed price')\n",
    "plt.plot(\n",
    "    np.arange(len(signal)),\n",
    "    scaled[:, 0],\n",
    "    'X',\n",
    "    label = 'outliers polarity based on close',\n",
    "    markevery = outliers,\n",
    "    c = 'r',\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(len(signal)),\n",
    "    scaled[:, 1],\n",
    "    'o',\n",
    "    label = 'outliers polarity based on close',\n",
    "    markevery = outliers,\n",
    "    c = 'r',\n",
    ")\n",
    "plt.xticks(\n",
    "    np.arange(len(signal))[::15], df['timestamp'][::15], rotation = 'vertical'\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesnt show much from trending, how about covariance correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.title('pearson correlation', y = 1.05, size = 16)\n",
    "\n",
    "mask = np.zeros_like(df.corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(\n",
    "    df.corr(),\n",
    "    mask = mask,\n",
    "    linewidths = 0.1,\n",
    "    vmax = 1.0,\n",
    "    square = True,\n",
    "    cmap = colormap,\n",
    "    linecolor = 'white',\n",
    "    annot = True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_shift(df, lag = 0, start = 1, skip = 1, rejected_columns = []):\n",
    "    df = df.copy()\n",
    "    if not lag:\n",
    "        return df\n",
    "    cols = {}\n",
    "    for i in range(start, lag + 1, skip):\n",
    "        for x in list(df.columns):\n",
    "            if x not in rejected_columns:\n",
    "                if not x in cols:\n",
    "                    cols[x] = ['{}_{}'.format(x, i)]\n",
    "                else:\n",
    "                    cols[x].append('{}_{}'.format(x, i))\n",
    "    for k, v in cols.items():\n",
    "        columns = v\n",
    "        dfn = pd.DataFrame(data = None, columns = columns, index = df.index)\n",
    "        i = 1\n",
    "        for c in columns:\n",
    "            dfn[c] = df[k].shift(periods = i)\n",
    "            i += 1\n",
    "        df = pd.concat([df, dfn], axis = 1, join_axes = [df.index])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_shift(df, lag = 42, start = 7, skip = 7)\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize = (30, 20))\n",
    "ax = plt.subplot(111)\n",
    "plt.title('42 hours correlation', y = 1.05, size = 16)\n",
    "selected_column = [\n",
    "    col\n",
    "    for col in list(df_new)\n",
    "    if any([k in col for k in ['Polarity', 'Sensitivity', 'Close']])\n",
    "]\n",
    "\n",
    "sns.heatmap(\n",
    "    df_new[selected_column].corr(),\n",
    "    ax = ax,\n",
    "    linewidths = 0.1,\n",
    "    vmax = 1.0,\n",
    "    square = True,\n",
    "    cmap = colormap,\n",
    "    linecolor = 'white',\n",
    "    annot = True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about we check trends from moving average? i chose 7, 14, 30 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think i had too much playing daily trending data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(signal, period):\n",
    "    buffer = [np.nan] * period\n",
    "    for i in range(period, len(signal)):\n",
    "        buffer.append(signal[i - period : i].mean())\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.copy(df['Close_Price'].values)\n",
    "ma_7 = moving_average(signal, 7)\n",
    "ma_14 = moving_average(signal, 14)\n",
    "ma_30 = moving_average(signal, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "plt.plot(np.arange(len(signal)), ma_7, label = 'ma 7')\n",
    "plt.plot(np.arange(len(signal)), ma_14, label = 'ma 14')\n",
    "plt.plot(np.arange(len(signal)), ma_30, label = 'ma 30')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trends gonna increase anyway!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now deep learning LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "learning_rate = 0.005\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 500\n",
    "dropout_rate = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(df.iloc[:, 0]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self, learning_rate, num_layers, size, size_layer, forget_bias = 0.8\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        self.logits = tf.layers.dense(\n",
    "            self.outputs[-1],\n",
    "            size,\n",
    "            kernel_initializer = tf.glorot_uniform_initializer(),\n",
    "        )\n",
    "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler().fit(\n",
    "    df[['Polarity', 'Sensitivity', 'Tweet_vol', 'Close_Price']].astype(\n",
    "        'float32'\n",
    "    )\n",
    ")\n",
    "df_scaled = minmax.transform(\n",
    "    df[['Polarity', 'Sensitivity', 'Tweet_vol', 'Close_Price']].astype(\n",
    "        'float32'\n",
    "    )\n",
    ")\n",
    "df_scaled = pd.DataFrame(df_scaled)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "modelnn = Model(\n",
    "    learning_rate, num_layers, df_scaled.shape[1], size_layer, dropout_rate\n",
    ")\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scale our data between 0 - 1 or any scaled you wanted, but must not less than -1 and more than 1, because LSTM is using tanh function, squashing high values can caused gradient vanishing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    total_loss = 0\n",
    "    for k in range(0, (df_scaled.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        batch_x = np.expand_dims(\n",
    "            df_scaled.iloc[k : k + timestamp].values, axis = 0\n",
    "        )\n",
    "        batch_y = df_scaled.iloc[k + 1 : k + timestamp + 1].values\n",
    "        last_state, _, loss = sess.run(\n",
    "            [modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "            feed_dict = {\n",
    "                modelnn.X: batch_x,\n",
    "                modelnn.Y: batch_y,\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        total_loss += loss\n",
    "    total_loss /= df.shape[0] // timestamp\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print('epoch:', i + 1, 'avg loss:', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(future_count, df, dates, indices = {}):\n",
    "    date_ori = dates[:]\n",
    "    cp_df = df.copy()\n",
    "    output_predict = np.zeros((cp_df.shape[0] + future_count, cp_df.shape[1]))\n",
    "    output_predict[0] = cp_df.iloc[0]\n",
    "    upper_b = (cp_df.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    for k in range(0, (df.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    cp_df.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "    out_logits, last_state = sess.run(\n",
    "        [modelnn.logits, modelnn.last_state],\n",
    "        feed_dict = {\n",
    "            modelnn.X: np.expand_dims(cp_df.iloc[upper_b:], axis = 0),\n",
    "            modelnn.hidden_layer: init_value,\n",
    "        },\n",
    "    )\n",
    "    init_value = last_state\n",
    "    output_predict[upper_b + 1 : cp_df.shape[0] + 1] = out_logits\n",
    "    cp_df.loc[cp_df.shape[0]] = out_logits[-1]\n",
    "    date_ori.append(date_ori[-1] + timedelta(hours = 1))\n",
    "    if indices:\n",
    "        for key, item in indices.items():\n",
    "            cp_df.iloc[-1, key] = item\n",
    "    for i in range(future_count - 1):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(cp_df.iloc[-timestamp:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[cp_df.shape[0]] = out_logits[-1]\n",
    "        cp_df.loc[cp_df.shape[0]] = out_logits[-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(hours = 1))\n",
    "        if indices:\n",
    "            for key, item in indices.items():\n",
    "                cp_df.iloc[-1, key] = item\n",
    "    return {'date_ori': date_ori, 'df': cp_df.values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some smoothing, using previous value as an anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_30 = predict_future(30, df_scaled, dates)\n",
    "predict_30['df'] = minmax.inverse_transform(predict_30['df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(\n",
    "    np.arange(len(predict_30['date_ori'])),\n",
    "    anchor(predict_30['df'][:, -1], 0.5),\n",
    "    label = 'predict signal',\n",
    ")\n",
    "plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happen if polarity is double from the max? Polarity is first index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_polarity = (minmax.data_max_[0] * 2 - minmax.data_min_[0]) / (\n",
    "    minmax.data_max_[0] - minmax.data_min_[0]\n",
    ")\n",
    "scaled_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "for retry in range(3):\n",
    "    plt.subplot(3, 1, retry + 1)\n",
    "    predict_30 = predict_future(\n",
    "        30, df_scaled, dates, indices = {0: scaled_polarity}\n",
    "    )\n",
    "    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n",
    "    plt.plot(\n",
    "        np.arange(len(predict_30['date_ori'])),\n",
    "        anchor(predict_30['df'][:, -1], 0.5),\n",
    "        label = 'predict signal',\n",
    "    )\n",
    "    plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I retried for 3 times just to study how fitted our model is, if every retry has big trend changes, so we need to retrain again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happen if polarity is quadriple from the max? Polarity is first index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_polarity = (minmax.data_max_[0] * 4 - minmax.data_min_[0]) / (\n",
    "    minmax.data_max_[0] - minmax.data_min_[0]\n",
    ")\n",
    "scaled_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "for retry in range(3):\n",
    "    plt.subplot(3, 1, retry + 1)\n",
    "    predict_30 = predict_future(\n",
    "        30, df_scaled, dates, indices = {0: scaled_polarity}\n",
    "    )\n",
    "    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n",
    "    plt.plot(\n",
    "        np.arange(len(predict_30['date_ori'])),\n",
    "        anchor(predict_30['df'][:, -1], 0.5),\n",
    "        label = 'predict signal',\n",
    "    )\n",
    "    plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happen if polarity is quadriple from the min? polarity is first index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_polarity = (minmax.data_min_[0] / 4 - minmax.data_min_[0]) / (\n",
    "    minmax.data_max_[0] - minmax.data_min_[0]\n",
    ")\n",
    "scaled_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "for retry in range(3):\n",
    "    plt.subplot(3, 1, retry + 1)\n",
    "    predict_30 = predict_future(\n",
    "        30, df_scaled, dates, indices = {0: scaled_polarity}\n",
    "    )\n",
    "    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n",
    "    plt.plot(\n",
    "        np.arange(len(predict_30['date_ori'])),\n",
    "        anchor(predict_30['df'][:, -1], 0.5),\n",
    "        label = 'predict signal',\n",
    "    )\n",
    "    plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second graph is skewed, but we got 2 graphs represented positive trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model learnt that, polarity gives negative correlation to the model. If polarity is increase, the trend is decreasing, vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happen if sentiment volume is double from the max? Volume is third index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_volume = (minmax.data_max_[2] * 2 - minmax.data_min_[2]) / (\n",
    "    minmax.data_max_[2] - minmax.data_min_[2]\n",
    ")\n",
    "scaled_volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "for retry in range(3):\n",
    "    plt.subplot(3, 1, retry + 1)\n",
    "    predict_30 = predict_future(\n",
    "        30, df_scaled, dates, indices = {2: scaled_volume}\n",
    "    )\n",
    "    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n",
    "    plt.plot(\n",
    "        np.arange(len(predict_30['date_ori'])),\n",
    "        anchor(predict_30['df'][:, -1], 0.5),\n",
    "        label = 'predict signal',\n",
    "    )\n",
    "    plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happen if sentiment volume is double from the min? Volume is third index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_volume = (minmax.data_min_[2] / 2 - minmax.data_min_[2]) / (\n",
    "    minmax.data_max_[2] - minmax.data_min_[2]\n",
    ")\n",
    "scaled_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "for retry in range(3):\n",
    "    plt.subplot(3, 1, retry + 1)\n",
    "    predict_30 = predict_future(\n",
    "        30, df_scaled, dates, indices = {2: scaled_volume}\n",
    "    )\n",
    "    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n",
    "    plt.plot(\n",
    "        np.arange(len(predict_30['date_ori'])),\n",
    "        anchor(predict_30['df'][:, -1], 0.5),\n",
    "        label = 'predict signal',\n",
    "    )\n",
    "    plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, volume does not brings any impact the learning so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
